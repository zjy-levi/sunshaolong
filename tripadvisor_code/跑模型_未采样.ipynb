{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一\n",
    "<!-- * PCA  -->\n",
    "bertopic 主题聚类 $\\to$ 因子分析 公因子\n",
    "* 因子分析（可解释） 5、6个\n",
    "* 4w $\\times$ p matrix\n",
    "## 分类模型\n",
    "smote采样/没有采样的\n",
    "    baseline model\n",
    "  * 逻辑回归\n",
    "  * svc\n",
    "  * DTC\n",
    "  * RF\n",
    "    跑一下baseline模型\n",
    "\n",
    "    进阶模型（我们提出的模型）\n",
    "  * CNN\n",
    "共10个模型\n",
    "## 评价准则\n",
    "精确率 召回率\n",
    "* CNN 采样/未采样/sharp value可解释性（可能是较好的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.078881</td>\n",
       "      <td>0.109237</td>\n",
       "      <td>0.121366</td>\n",
       "      <td>0.081104</td>\n",
       "      <td>0.085179</td>\n",
       "      <td>1.525199</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.019840</td>\n",
       "      <td>-0.017074</td>\n",
       "      <td>0.117229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401077</td>\n",
       "      <td>0.244692</td>\n",
       "      <td>-0.423489</td>\n",
       "      <td>-0.252544</td>\n",
       "      <td>-0.104657</td>\n",
       "      <td>0.061354</td>\n",
       "      <td>0.212366</td>\n",
       "      <td>-0.106895</td>\n",
       "      <td>-0.174594</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.129565</td>\n",
       "      <td>1.324284</td>\n",
       "      <td>1.503223</td>\n",
       "      <td>1.182611</td>\n",
       "      <td>1.149371</td>\n",
       "      <td>20.243247</td>\n",
       "      <td>0.036048</td>\n",
       "      <td>0.587170</td>\n",
       "      <td>0.579361</td>\n",
       "      <td>1.030450</td>\n",
       "      <td>...</td>\n",
       "      <td>3.122662</td>\n",
       "      <td>1.930230</td>\n",
       "      <td>-2.236521</td>\n",
       "      <td>-1.629814</td>\n",
       "      <td>-0.415996</td>\n",
       "      <td>-0.191336</td>\n",
       "      <td>0.942498</td>\n",
       "      <td>-0.306906</td>\n",
       "      <td>-0.766555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113380</td>\n",
       "      <td>0.137732</td>\n",
       "      <td>0.187015</td>\n",
       "      <td>0.117512</td>\n",
       "      <td>0.129966</td>\n",
       "      <td>0.981701</td>\n",
       "      <td>0.141062</td>\n",
       "      <td>0.040679</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.060275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656048</td>\n",
       "      <td>0.285357</td>\n",
       "      <td>-0.509809</td>\n",
       "      <td>-0.417252</td>\n",
       "      <td>-0.088661</td>\n",
       "      <td>-0.030604</td>\n",
       "      <td>0.214359</td>\n",
       "      <td>0.052981</td>\n",
       "      <td>-0.053427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130303</td>\n",
       "      <td>0.154464</td>\n",
       "      <td>0.197254</td>\n",
       "      <td>0.134266</td>\n",
       "      <td>0.144115</td>\n",
       "      <td>0.983246</td>\n",
       "      <td>0.144722</td>\n",
       "      <td>0.047023</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.020829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657501</td>\n",
       "      <td>0.272685</td>\n",
       "      <td>-0.516043</td>\n",
       "      <td>-0.421925</td>\n",
       "      <td>-0.081141</td>\n",
       "      <td>-0.025779</td>\n",
       "      <td>0.218346</td>\n",
       "      <td>0.061396</td>\n",
       "      <td>-0.057581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117511</td>\n",
       "      <td>0.128078</td>\n",
       "      <td>0.254737</td>\n",
       "      <td>0.128391</td>\n",
       "      <td>0.151780</td>\n",
       "      <td>0.334925</td>\n",
       "      <td>0.328880</td>\n",
       "      <td>0.037059</td>\n",
       "      <td>-0.013657</td>\n",
       "      <td>0.211102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940115</td>\n",
       "      <td>0.429599</td>\n",
       "      <td>-0.581791</td>\n",
       "      <td>-0.879451</td>\n",
       "      <td>-0.135077</td>\n",
       "      <td>-0.183515</td>\n",
       "      <td>0.245517</td>\n",
       "      <td>0.520213</td>\n",
       "      <td>0.172435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4          5         6  \\\n",
       "0  0.078881  0.109237  0.121366  0.081104  0.085179   1.525199  0.016275   \n",
       "1  1.129565  1.324284  1.503223  1.182611  1.149371  20.243247  0.036048   \n",
       "2  0.113380  0.137732  0.187015  0.117512  0.129966   0.981701  0.141062   \n",
       "3  0.130303  0.154464  0.197254  0.134266  0.144115   0.983246  0.144722   \n",
       "4  0.117511  0.128078  0.254737  0.128391  0.151780   0.334925  0.328880   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0  0.019840 -0.017074  0.117229  ...  0.401077  0.244692 -0.423489 -0.252544   \n",
       "1  0.587170  0.579361  1.030450  ...  3.122662  1.930230 -2.236521 -1.629814   \n",
       "2  0.040679  0.001235  0.060275  ...  0.656048  0.285357 -0.509809 -0.417252   \n",
       "3  0.047023  0.004954  0.020829  ...  0.657501  0.272685 -0.516043 -0.421925   \n",
       "4  0.037059 -0.013657  0.211102  ...  0.940115  0.429599 -0.581791 -0.879451   \n",
       "\n",
       "         15        16        17        18        19  label  \n",
       "0 -0.104657  0.061354  0.212366 -0.106895 -0.174594      1  \n",
       "1 -0.415996 -0.191336  0.942498 -0.306906 -0.766555      0  \n",
       "2 -0.088661 -0.030604  0.214359  0.052981 -0.053427      1  \n",
       "3 -0.081141 -0.025779  0.218346  0.061396 -0.057581      1  \n",
       "4 -0.135077 -0.183515  0.245517  0.520213  0.172435      1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入数据\n",
    "import pandas as pd\n",
    "rd=pd.read_csv('E:\\code\\python_code\\BDManager\\sunshaolong\\model\\因子分析20维数据.csv',index_col=0)\n",
    "rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=rd.iloc[:,:-1]\n",
    "y=rd.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    41493\n",
       "0     1649\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([X,y],axis=1).to_csv('E:\\code\\python_code\\BDManager\\sunshaolong\\model\\因子分析20维数据.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    41493\n",
       "-1     1649\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7240\\2563976117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 划分训练与测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# 划分训练与测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anoconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;std&#x27;, StandardScaler()), (&#x27;logit&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;std&#x27;, StandardScaler()), (&#x27;logit&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('std', StandardScaler()), ('logit', LogisticRegression())])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "pip=Pipeline(\n",
    "    [\n",
    "        ('std',StandardScaler()),\n",
    "        ('logit',LogisticRegression())\n",
    "    ]\n",
    ")\n",
    "# logit=LogisticRegression()\n",
    "pip.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623363077992815"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    8311\n",
       "-1     318\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_neg=X.loc[y_test[y_test==-1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018867924528301886"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip.score(X_test_neg,y_test[y_test==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出未采样条件下对于负类的识别正确率仅为1%，样本极度不平衡的情况也导致了正确率虚高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.支持向量分类器（SVC）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\code\\python_code\\BDManager\\sunshaolong\\tripadvisor_code\\跑模型_未采样.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/code/python_code/BDManager/sunshaolong/tripadvisor_code/%E8%B7%91%E6%A8%A1%E5%9E%8B_%E6%9C%AA%E9%87%87%E6%A0%B7.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/code/python_code/BDManager/sunshaolong/tripadvisor_code/%E8%B7%91%E6%A8%A1%E5%9E%8B_%E6%9C%AA%E9%87%87%E6%A0%B7.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     pip\u001b[39m.\u001b[39mset_params(svc__degree\u001b[39m=\u001b[39mi)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/code/python_code/BDManager/sunshaolong/tripadvisor_code/%E8%B7%91%E6%A8%A1%E5%9E%8B_%E6%9C%AA%E9%87%87%E6%A0%B7.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     pip\u001b[39m.\u001b[39;49mfit(X_train,y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/code/python_code/BDManager/sunshaolong/tripadvisor_code/%E8%B7%91%E6%A8%A1%E5%9E%8B_%E6%9C%AA%E9%87%87%E6%A0%B7.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m=================degree:\u001b[39m\u001b[39m'\u001b[39m,i,\u001b[39m'\u001b[39m\u001b[39m=================\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/code/python_code/BDManager/sunshaolong/tripadvisor_code/%E8%B7%91%E6%A8%A1%E5%9E%8B_%E6%9C%AA%E9%87%87%E6%A0%B7.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m测试集上的得分：\u001b[39m\u001b[39m'\u001b[39m,pip\u001b[39m.\u001b[39mscore(X_test,y_test))\n",
      "File \u001b[1;32mE:\\Anoconda\\Lib\\site-packages\\sklearn\\pipeline.py:382\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    381\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 382\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mE:\\Anoconda\\Lib\\site-packages\\sklearn\\svm\\_base.py:251\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    250\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 251\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    252\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mE:\\Anoconda\\Lib\\site-packages\\sklearn\\svm\\_base.py:333\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    319\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    321\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    323\u001b[0m (\n\u001b[0;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    330\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    331\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[0;32m    332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[1;32m--> 333\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    334\u001b[0m     X,\n\u001b[0;32m    335\u001b[0m     y,\n\u001b[0;32m    336\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[0;32m    337\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    338\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight_,\n\u001b[0;32m    339\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    340\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    341\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    342\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[0;32m    343\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    344\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[0;32m    345\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    346\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    347\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    348\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    349\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    350\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    351\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "pip=Pipeline(\n",
    "    [\n",
    "        ('std',StandardScaler()),\n",
    "        ('svc',SVC())\n",
    "    ]\n",
    ")\n",
    "# logit=LogisticRegression()\n",
    "for i in range(1,4):\n",
    "    pip.set_params(svc__degree=i)\n",
    "    pip.fit(X_train,y_train)\n",
    "    print('=================degree:',i,'=================')\n",
    "    print('测试集上的得分：',pip.score(X_test,y_test))\n",
    "    print('测试集上负例的得分：',pip.score(X_test_neg,y_test[y_test==-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的得分： 0.9457642832309654\n",
      "测试集上负例的得分： 0.31446540880503143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "print('测试集上的得分：',dt.score(X_test,y_test))\n",
    "print('测试集上负例的得分：',dt.score(X_test_neg,y_test[y_test==-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上的得分： 0.9634951906362267\n",
      "测试集上负例的得分： 0.1949685534591195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "print('测试集上的得分：',rfc.score(X_test,y_test))\n",
    "print('测试集上负例的得分：',rfc.score(X_test_neg,y_test[y_test==-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# F.max_pool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class  Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # nn.Module子类的函数必须在构造函数中执行父类的构造函数\n",
    "        # 下式等价于nn.Module.__init__(self)\n",
    "        super(Net, self).__init__() # 继承Net子类的超类nn.Module\n",
    "        # 卷积层 1表示输入为单通道，6表示输出通道数，5表示卷积核为5*5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # 卷积层\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 仿射层/全联接层/线性层\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #卷积—>激活—>池化\n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # reshape ,’-1’表示自适应\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=t.arange(10)\n",
    "b.resize_(2,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze_(2)\n",
    "# b=b.view(2,5)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  5],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "a=t.arange(12).resize(2,2,3)\n",
    "a[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  1],\n",
       "         [ 3,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 9, 10]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.gather(2,t.LongTensor([[[2,1],\n",
    "                            [0,2],\n",
    "                            ],[[0,1],[0,1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x=Variable(t.ones(4),requires_grad=True)\n",
    "# x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=y*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4., 4.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始搞cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch.autograd import  Variable as V\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练与测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class TopicvecSet(Dataset):\n",
    "    def __init__(self,train = True) -> None:\n",
    "        super(TopicvecSet, self).__init__()\n",
    "        if train == True:\n",
    "            self.X = t.from_numpy(X_train.to_numpy()).float().view(-1,20) # 矩阵类型的\n",
    "            self.X.requires_grad_(True)\n",
    "            self.y =  t.from_numpy(y_train.to_numpy()).float().unsqueeze(1) # 必须是矩阵类型的\n",
    "        else :\n",
    "            self.X = t.from_numpy(X_test.to_numpy()).float().view(-1,20)\n",
    "            self.X.requires_grad_(True)\n",
    "            self.y =  t.from_numpy(y_test.to_numpy()).float().unsqueeze(1)\n",
    "        self.len = self.X.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return t.index_select(self.X,dim=0,index = t.LongTensor([index])), self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "train_dataset = TopicvecSet(train=True) #取训练集\n",
    "test_dataset = TopicvecSet(train=False)\n",
    "train_loader = DataLoader(dataset =train_dataset, batch_size=2 , shuffle = False, drop_last=True) # \n",
    "test_loader = DataLoader(dataset= test_dataset, batch_size=1, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34513, 20])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.from_numpy(X_train.to_numpy()).float().view(-1,20).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义cnn网络\n",
    "\n",
    "class myCNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(myCNN,self).__init__()\n",
    "        # 第一层 卷积\n",
    "        layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1,16,3,padding=1), #input 1, output 16, k_size 3 -->16, 20, 1\n",
    "            # print(x.shape),\n",
    "            nn.ReLU(True), #relu acitvate\n",
    "            nn.MaxPool1d(2) #pool一下 --> 16, 10, 1\n",
    "        )\n",
    "        self.layer1 = layer1\n",
    "        layer2 = nn.Sequential(\n",
    "            nn.Conv1d(16,32,3,1,1), #--> 32, 10,1\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool1d(2) #-->32, 5,1\n",
    "        )\n",
    "        self.layer2 = layer2\n",
    "        layer3 = nn.Sequential(\n",
    "            nn.Linear(160, 80), \n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(80,40),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(40,1)\n",
    "        )\n",
    "        self.layer3 = layer3\n",
    "\n",
    "    def  forward(self, x):\n",
    "        # print(x.shape)\n",
    "        batch_size = x.size(0)\n",
    "        conv1 = self.layer1(x)\n",
    "        conv2 = self.layer2(conv1)\n",
    "        output = t.sigmoid(self.layer3(conv2.view(batch_size,-1)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU上定义loss和优化器\n",
    "mycnn = myCNN()\n",
    "# device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n",
    "# t.cuda.empty_cache()\n",
    "# mycnn.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(params=mycnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义训练过程\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate (train_loader, 0):\n",
    "        inputs, target = data\n",
    "        # inputs, target = inputs.to(device), target.to(device)\n",
    "        # forward + backward + update\n",
    "        # forward 算loss\n",
    "        outputs = mycnn(inputs)\n",
    "        # print(outputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        # print (epoch, loss.item())\n",
    "        # backward 算梯度\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update 更新参数\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print(f'第{epoch+1}次epoch--->第{batch_idx+1}次batch--->损失为：{running_loss/300}')\n",
    "            running_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试过程\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_neg = 0### neg测试\n",
    "    correct_neg = 0 ###neg测试\n",
    "    with t.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            # inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = mycnn(inputs)\n",
    "            outputs.apply_(lambda x:1.0 if outputs.item()>0.5 else 0.0)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "            total += outputs.size(0)\n",
    "            label_neg = labels[labels==0]\n",
    "            outputs_neg = outputs[t.where(labels==0)]\n",
    "            correct_neg += (outputs_neg == label_neg).sum().item()\n",
    "            total_neg += outputs_neg.size(0)\n",
    "    print('测试集上的准确率为：',correct/(1.0*total))\n",
    "    print('负例正确率为：',correct_neg/(1.0*total_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次epoch--->第300次batch--->损失为：0.09865885697232443\n",
      "第1次epoch--->第600次batch--->损失为：0.1330728701587456\n",
      "第1次epoch--->第900次batch--->损失为：0.11698024263392047\n",
      "第1次epoch--->第1200次batch--->损失为：0.12009250415091326\n",
      "第1次epoch--->第1500次batch--->损失为：0.1074587033650217\n",
      "第1次epoch--->第1800次batch--->损失为：0.11618712669316059\n",
      "第1次epoch--->第2100次batch--->损失为：0.08002200182585512\n",
      "第1次epoch--->第2400次batch--->损失为：0.10620969312420736\n",
      "第1次epoch--->第2700次batch--->损失为：0.10541867592701844\n",
      "第1次epoch--->第3000次batch--->损失为：0.10422074189021563\n",
      "第1次epoch--->第3300次batch--->损失为：0.12409913616759392\n",
      "第1次epoch--->第3600次batch--->损失为：0.08012452236085664\n",
      "第1次epoch--->第3900次batch--->损失为：0.11353842438955325\n",
      "第1次epoch--->第4200次batch--->损失为：0.08957111701194663\n",
      "第1次epoch--->第4500次batch--->损失为：0.11532135141621741\n",
      "第1次epoch--->第4800次batch--->损失为：0.11052495415594119\n",
      "第1次epoch--->第5100次batch--->损失为：0.13113158114506707\n",
      "第1次epoch--->第5400次batch--->损失为：0.12971899230324196\n",
      "第1次epoch--->第5700次batch--->损失为：0.10150510703814992\n",
      "第1次epoch--->第6000次batch--->损失为：0.12198114233634745\n",
      "第1次epoch--->第6300次batch--->损失为：0.12969817536727835\n",
      "第1次epoch--->第6600次batch--->损失为：0.0979821758106118\n",
      "第1次epoch--->第6900次batch--->损失为：0.16984886030860555\n",
      "第1次epoch--->第7200次batch--->损失为：0.07586643734112537\n",
      "第1次epoch--->第7500次batch--->损失为：0.09160135520849508\n",
      "第1次epoch--->第7800次batch--->损失为：0.09202010586355755\n",
      "第1次epoch--->第8100次batch--->损失为：0.11234870993988201\n",
      "第1次epoch--->第8400次batch--->损失为：0.12710234009039897\n",
      "第1次epoch--->第8700次batch--->损失为：0.10641409497281226\n",
      "第1次epoch--->第9000次batch--->损失为：0.11579628445596124\n",
      "第1次epoch--->第9300次batch--->损失为：0.07858874832978473\n",
      "第1次epoch--->第9600次batch--->损失为：0.12457645963256558\n",
      "第1次epoch--->第9900次batch--->损失为：0.11314488369505853\n",
      "第1次epoch--->第10200次batch--->损失为：0.08457026135157018\n",
      "第1次epoch--->第10500次batch--->损失为：0.09371724831891091\n",
      "第1次epoch--->第10800次batch--->损失为：0.13241765832082214\n",
      "第1次epoch--->第11100次batch--->损失为：0.1019284288273775\n",
      "第1次epoch--->第11400次batch--->损失为：0.10216429242995219\n",
      "第1次epoch--->第11700次batch--->损失为：0.1446338054188527\n",
      "第1次epoch--->第12000次batch--->损失为：0.12356677644924882\n",
      "第1次epoch--->第12300次batch--->损失为：0.12024609698875187\n",
      "第1次epoch--->第12600次batch--->损失为：0.13586430310698536\n",
      "第1次epoch--->第12900次batch--->损失为：0.12035367992726\n",
      "第1次epoch--->第13200次batch--->损失为：0.06923220912110992\n",
      "第1次epoch--->第13500次batch--->损失为：0.10714374057521733\n",
      "第1次epoch--->第13800次batch--->损失为：0.11874504017604826\n",
      "第1次epoch--->第14100次batch--->损失为：0.11580731048015877\n",
      "第1次epoch--->第14400次batch--->损失为：0.10402977476711385\n",
      "第1次epoch--->第14700次batch--->损失为：0.14060451215482317\n",
      "第1次epoch--->第15000次batch--->损失为：0.07319122058300612\n",
      "第1次epoch--->第15300次batch--->损失为：0.15708565758774057\n",
      "第1次epoch--->第15600次batch--->损失为：0.07734962916137496\n",
      "第1次epoch--->第15900次batch--->损失为：0.09166793533986492\n",
      "第1次epoch--->第16200次batch--->损失为：0.1022008694326117\n",
      "第1次epoch--->第16500次batch--->损失为：0.10997562517188877\n",
      "第1次epoch--->第16800次batch--->损失为：0.12565272339154035\n",
      "第1次epoch--->第17100次batch--->损失为：0.11956569236227854\n",
      "测试集上的准确率为： 0.9632634140688376\n",
      "负例正确率为： 0.0031446540880503146\n",
      "第2次epoch--->第300次batch--->损失为：0.09543551604496316\n",
      "第2次epoch--->第600次batch--->损失为：0.13021847660827915\n",
      "第2次epoch--->第900次batch--->损失为：0.1169699176895665\n",
      "第2次epoch--->第1200次batch--->损失为：0.113772731142332\n",
      "第2次epoch--->第1500次batch--->损失为：0.10529654534339594\n",
      "第2次epoch--->第1800次batch--->损失为：0.11372381237878774\n",
      "第2次epoch--->第2100次batch--->损失为：0.08018133218982257\n",
      "第2次epoch--->第2400次batch--->损失为：0.10630847872555023\n",
      "第2次epoch--->第2700次batch--->损失为：0.10277641592848037\n",
      "第2次epoch--->第3000次batch--->损失为：0.1043315972952405\n",
      "第2次epoch--->第3300次batch--->损失为：0.11953864344444204\n",
      "第2次epoch--->第3600次batch--->损失为：0.07636413823192319\n",
      "第2次epoch--->第3900次batch--->损失为：0.11170641829017161\n",
      "第2次epoch--->第4200次batch--->损失为：0.08769985348413077\n",
      "第2次epoch--->第4500次batch--->损失为：0.11618269245192399\n",
      "第2次epoch--->第4800次batch--->损失为：0.10773492404647793\n",
      "第2次epoch--->第5100次batch--->损失为：0.1303630896228909\n",
      "第2次epoch--->第5400次batch--->损失为：0.1285802504804451\n",
      "第2次epoch--->第5700次batch--->损失为：0.09905923805335382\n",
      "第2次epoch--->第6000次batch--->损失为：0.12008843040675857\n",
      "第2次epoch--->第6300次batch--->损失为：0.12604501393031872\n",
      "第2次epoch--->第6600次batch--->损失为：0.09835139299102594\n",
      "第2次epoch--->第6900次batch--->损失为：0.16982193953357638\n",
      "第2次epoch--->第7200次batch--->损失为：0.07262502547489325\n",
      "第2次epoch--->第7500次batch--->损失为：0.09455148084496613\n",
      "第2次epoch--->第7800次batch--->损失为：0.08859428661812369\n",
      "第2次epoch--->第8100次batch--->损失为：0.10666687861841638\n",
      "第2次epoch--->第8400次batch--->损失为：0.12577411029390836\n",
      "第2次epoch--->第8700次batch--->损失为：0.10325134737281284\n",
      "第2次epoch--->第9000次batch--->损失为：0.11436047134377683\n",
      "第2次epoch--->第9300次batch--->损失为：0.07835076823326138\n",
      "第2次epoch--->第9600次batch--->损失为：0.12457764913696641\n",
      "第2次epoch--->第9900次batch--->损失为：0.11164058288016046\n",
      "第2次epoch--->第10200次batch--->损失为：0.0835009774400775\n",
      "第2次epoch--->第10500次batch--->损失为：0.09238041958267179\n",
      "第2次epoch--->第10800次batch--->损失为：0.13144681420705942\n",
      "第2次epoch--->第11100次batch--->损失为：0.10075770914224752\n",
      "第2次epoch--->第11400次batch--->损失为：0.10043848223807193\n",
      "第2次epoch--->第11700次batch--->损失为：0.14423590372122513\n",
      "第2次epoch--->第12000次batch--->损失为：0.12227810003755925\n",
      "第2次epoch--->第12300次batch--->损失为：0.11675756766984705\n",
      "第2次epoch--->第12600次batch--->损失为：0.13589304828395446\n",
      "第2次epoch--->第12900次batch--->损失为：0.11981278607602386\n",
      "第2次epoch--->第13200次batch--->损失为：0.06662747632498697\n",
      "第2次epoch--->第13500次batch--->损失为：0.10535139445448294\n",
      "第2次epoch--->第13800次batch--->损失为：0.11761875804377875\n",
      "第2次epoch--->第14100次batch--->损失为：0.11447521928309773\n",
      "第2次epoch--->第14400次batch--->损失为：0.10272029907015773\n",
      "第2次epoch--->第14700次batch--->损失为：0.13966499723726883\n",
      "第2次epoch--->第15000次batch--->损失为：0.0728470294601478\n",
      "第2次epoch--->第15300次batch--->损失为：0.1553303344155817\n",
      "第2次epoch--->第15600次batch--->损失为：0.0761653539990463\n",
      "第2次epoch--->第15900次batch--->损失为：0.09182227678509662\n",
      "第2次epoch--->第16200次batch--->损失为：0.10337350297631928\n",
      "第2次epoch--->第16500次batch--->损失为：0.10791025657420202\n",
      "第2次epoch--->第16800次batch--->损失为：0.1235346678388305\n",
      "第2次epoch--->第17100次batch--->损失为：0.12121401139496205\n",
      "测试集上的准确率为： 0.9632634140688376\n",
      "负例正确率为： 0.012578616352201259\n",
      "第3次epoch--->第300次batch--->损失为：0.09280707255607315\n",
      "第3次epoch--->第600次batch--->损失为：0.12840051935173202\n",
      "第3次epoch--->第900次batch--->损失为：0.11609596980774464\n",
      "第3次epoch--->第1200次batch--->损失为：0.11252260808798989\n",
      "第3次epoch--->第1500次batch--->损失为：0.10496660238325907\n",
      "第3次epoch--->第1800次batch--->损失为：0.11262231275904924\n",
      "第3次epoch--->第2100次batch--->损失为：0.08100058164680377\n",
      "第3次epoch--->第2400次batch--->损失为：0.10449707351489147\n",
      "第3次epoch--->第2700次batch--->损失为：0.09964712277326422\n",
      "第3次epoch--->第3000次batch--->损失为：0.10388619476749833\n",
      "第3次epoch--->第3300次batch--->损失为：0.11890134045349744\n",
      "第3次epoch--->第3600次batch--->损失为：0.07533631908891646\n",
      "第3次epoch--->第3900次batch--->损失为：0.11081034293990039\n",
      "第3次epoch--->第4200次batch--->损失为：0.08566371838756216\n",
      "第3次epoch--->第4500次batch--->损失为：0.11516515224889493\n",
      "第3次epoch--->第4800次batch--->损失为：0.10670125520477693\n",
      "第3次epoch--->第5100次batch--->损失为：0.13071688210446156\n",
      "第3次epoch--->第5400次batch--->损失为：0.12805756551019537\n",
      "第3次epoch--->第5700次batch--->损失为：0.09795934545031439\n",
      "第3次epoch--->第6000次batch--->损失为：0.1196664349688217\n",
      "第3次epoch--->第6300次batch--->损失为：0.12457586579529258\n",
      "第3次epoch--->第6600次batch--->损失为：0.09830636650479088\n",
      "第3次epoch--->第6900次batch--->损失为：0.16866007062024438\n",
      "第3次epoch--->第7200次batch--->损失为：0.07152612217139298\n",
      "第3次epoch--->第7500次batch--->损失为：0.09108231591366348\n",
      "第3次epoch--->第7800次batch--->损失为：0.08716422735646726\n",
      "第3次epoch--->第8100次batch--->损失为：0.10432044338071136\n",
      "第3次epoch--->第8400次batch--->损失为：0.12391545860737097\n",
      "第3次epoch--->第8700次batch--->损失为：0.10111588806088548\n",
      "第3次epoch--->第9000次batch--->损失为：0.11218714223941788\n",
      "第3次epoch--->第9300次batch--->损失为：0.07835681812275046\n",
      "第3次epoch--->第9600次batch--->损失为：0.12264261756557972\n",
      "第3次epoch--->第9900次batch--->损失为：0.11077254607109353\n",
      "第3次epoch--->第10200次batch--->损失为：0.08271225621642467\n",
      "第3次epoch--->第10500次batch--->损失为：0.09210728226326562\n",
      "第3次epoch--->第10800次batch--->损失为：0.131460125859788\n",
      "第3次epoch--->第11100次batch--->损失为：0.09979737690999173\n",
      "第3次epoch--->第11400次batch--->损失为：0.09878328086577919\n",
      "第3次epoch--->第11700次batch--->损失为：0.14350510718262133\n",
      "第3次epoch--->第12000次batch--->损失为：0.12121274574640362\n",
      "第3次epoch--->第12300次batch--->损失为：0.11431295104053182\n",
      "第3次epoch--->第12600次batch--->损失为：0.1357682653477726\n",
      "第3次epoch--->第12900次batch--->损失为：0.11855925942848747\n",
      "第3次epoch--->第13200次batch--->损失为：0.06606388823556093\n",
      "第3次epoch--->第13500次batch--->损失为：0.10440289403729063\n",
      "第3次epoch--->第13800次batch--->损失为：0.11543635555310175\n",
      "第3次epoch--->第14100次batch--->损失为：0.11334069006533051\n",
      "第3次epoch--->第14400次batch--->损失为：0.10337697763500425\n",
      "第3次epoch--->第14700次batch--->损失为：0.13942147769227933\n",
      "第3次epoch--->第15000次batch--->损失为：0.07226589400951829\n",
      "第3次epoch--->第15300次batch--->损失为：0.1537503753043711\n",
      "第3次epoch--->第15600次batch--->损失为：0.07578196271749524\n",
      "第3次epoch--->第15900次batch--->损失为：0.09171878198443058\n",
      "第3次epoch--->第16200次batch--->损失为：0.10234893379732966\n",
      "第3次epoch--->第16500次batch--->损失为：0.10761709516334425\n",
      "第3次epoch--->第16800次batch--->损失为：0.12347297444241122\n",
      "第3次epoch--->第17100次batch--->损失为：0.12047228504399148\n",
      "测试集上的准确率为： 0.9633793023525321\n",
      "负例正确率为： 0.015723270440251572\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(3):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = t.tensor(y_test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label[label==0]\n",
    "label[t.where(label==0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86a02abacfdced7cbb0389647678172009018360f51ccf826ae31803238824eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
